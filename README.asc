Swift cloud on Amazon Elastic Compute Cloud
-------------------------------------------

Sign up online
^^^^^^^^^^^^^^

Sign up for Amazon Web Services (AWS) and create an IAM user by following instructions
from http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html[Amazon's Setup Documentation].
From the documentation you would only need to do the following two steps:

1. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html#sign-up-for-aws[Sign up for AWS]

2. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html#create-an-iam-user[Create and IAM User]

NOTE: To create access keys, you must have permissions to perform the required
IAM actions. For more information, see http://docs.aws.amazon.com/IAM/latest/UserGuide/ManagingCredentials.html[Managing Credentials using IAM].

NOTE: For this tutorial, use the AWS zone US West Oregon.

If you already have an account, here's the steps to get you IAM access keys:
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

1. Go to the IAM console.

2. From the navigation menu, click Users.

3. Select your IAM user name, or create a new one.

4. Click User Actions, and then click Manage Access Keys.

5. Click Create Access Key.

6. Click Download Credentials, and store the keys (a .csv file) in a secure location.

NOTE: Your secret key will no longer be available through the AWS Management Console;
you will have the only copy. Keep it confidential in order to protect your account.

NOTE: Your keys will look something like this: +
Access key ID example    : AKIAIOSFODNN7*EXAMPLE* +
Secret access key example: wJalrXUtnFEMI/K7MDENG/bPxRfiCY*EXAMPLEKEY* +


Create a Keypair so that you can ssh to the ec2 instances
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++

1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.

2. In the top navigation bar, in the region selector, click *US West (Oregon)*.

3. In the left navigation pane, under Network and Security, click Key Pairs.

4. Click Create Key Pair.

5. Type mykeypair in the new Key pair name box, and then click Yes.

6. Download the private key file, which is named mykeypair.pem, and keep it in a safe
   place. You will need it to access any instances that you launch with this key pair.

Here's http://docs.aws.amazon.com/gettingstarted/latest/wah/getting-started-create-key-pair.html[Amazon's documentation] on Key Pairs.

Launching an EC2 Instance
+++++++++++++++++++++++++

Here's the comprehensive http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launching-instance.html[documentation] from Amazon on launching instances.

Steps to follow :

1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ .

2. In the navigation bar at the top of the screen, the current region is displayed. Select the region *US West (Oregon) for the instance.

3. From the Amazon EC2 console dashboard, click Launch Instance. (Blue button under Create Instance, middle panel)

4. On the Choose an Amazon Machine Image (AMI) page, choose an AMI as follows:
   Choose the following AMI and then click Select.
   Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-33db9803

5. On the Choose an Instance Type page, select the hardware configuration and size of the instance to launch.
   For this tutorial preferably use instance type t2.micro / t2.small.

6. You can skip over Add Storage

7. Go to Tag Instance and name your instance as *launchpad*. We will refer to this machine from here on as launchpad.

8. Under Configure Security Group, you will see that ssh is allowed from anywhere. You need not modify anything here.

9. Go to Review and click the launch button.

10. You will see a dialog box, select the key pair that you created in the create Key-pair step in the previous section,
    check the box, and click Launch instances.

Note: At times some instance types may not be available in the Oregon region. In such cases, go back to the review screen
and change the instance type, and retry launch.

Connect to launchpad
++++++++++++++++++++

1. Go to the EC2 console https://console.aws.amazon.com/ec2/
2. Click on Instances under INSTANCES on the left pane.
3. You can see the instance named launchpad running, click on the row to see a description of the instance
    in a lower panel.
4. Get the public-ip of launchpad.
5. Locate the .pem key downloaded from the create Key-pair step (probably your default downloads folder)
   On linux use chmod to change permissions of the key ( chmod 400 mykeypair.pem )

6. ssh to launchpad with :
    [source,bash]
    -----
    ssh -i /path/to/mykeypair.pem ubuntu@<PUBLIC_IP_OF_LAUNCHPAD>
    -----

7. Use scp to copy the keypair and the IAM access credentials file to launchpad on your local machine:
    [source,bash]
    -----
    # Copy the ssh keypair file to the home directory of the launchpad instance
    scp -i /path/to/mykeypair.pem /path/to/mykeypair.pem ubuntu@<PUBLIC_IP_OF_LAUNCHPAD>:~/
    # Copy the IAM credentials file to the home directory of the launchpad instance
    scp -i /path/to/mykeypair.pem /path/to/credentials.csv ubuntu@<PUBLIC_IP_OF_LAUNCHPAD>:~/
    -----


Setting up launchpad instance
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Install python and libcloud library. Since we are using an Ubuntu instance:

[source,bash]
-----
sudo apt-get update
sudo apt-get install -y python python-pip git
sudo pip install apache-libcloud
-----

Get the cloud-tutorials repository from git
[source,bash]
-----
git clone https://github.com/yadudoc/cloud-tutorials.git
cd cloud-tutorials/ec2
-----

From the launchpad, we will be starting a cloud instance which we call *headnode* and any number
of instances called "worker" nodes. The swift service runs on the headnode, and the tasks involved
in the swift workflow are sent to the worker nodes. The headnode is where we will run the swift scripts.

Manage you Cloud resources
^^^^^^^^^^^^^^^^^^^^^^^^^^

Update the file cloud-tutorials/ec2/configs in the cloned repository from the previous step,
with the following information:

 *  AWS_CREDENTIALS_FILE : Path to the credentials file downloaded in step 6 of _sign_up_online
 *  AWS_WORKER_COUNT : Use this to specify the number of worker instances required.
 *  AWS_KEYPAIR_NAME : Name of the keypair to use. If this keypair does not exist a keypair of the same name will be generated.
 *  AWS_KEYPAIR_FILE : Path to the <keypair>.pem. If a keypair file of the specified name does not exist, a new one will be generated.
 *  AWS_USERNAME     : The username used for login to the cloud-instances. Set to the default "ec2-user" for Amazon Linux AMI.
 *  AWS_REGION       : Default=us-west-2 | Do NOT change
 *  SECURITY_GROUP   : Default=swift_security_group1
 *  HEADNODE_IMAGE, WORKER_IMAGE : These are images used to boot up the headnode and workers.
 *  HEADNODE_MACHINE_TYPE, WORKER_MACHINE_TYPE : VM types for workers and the headnode. Choose between :
    - t2.<micro/small/medium>, m3.<medium,large,xlarge,2xlarge> :For general purpose computing
    - c3.<,x,2x,4x,8x>large             : Compute Optimised
    - g2.2xlarge                        : GPU Instances
    - r3.<,x,2x,4x,8x>large             : Memory Optimised
    - i2.<x,2x,4x,8x>large, hs1.8xlarge : Storage Optimised
 *  WORKER_INIT_SCRIPT: You can specify a bash script here which would be executed along with the cloud-init script. This would
                        allow you to specify install and config scripts to setup the worker instances at boot time.

NOTE: Get details of instances and pricing http://aws.amazon.com/ec2/pricing[here]. The pricing varies
between geographical locations so make sure you check for US West Oregon, which is the default.

NOTE: AWS_WORKER_COUNT directly affects the cost. If you require more than 22 nodes including the
headnode, file a request to increase your resource quotas.

NOTE: Read more about AWS regions and availability zones http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html[here]

WARNING:  Do *NOT* change the images for the worker and headnode to point at images which have not
been explicitly setup with swift.

Start your cloud setup!
^^^^^^^^^^^^^^^^^^^^^^^

Once you finish editing the configs file with your preferences, start the cloud instances
by sourcing the setup script.

The setup script will setup firewall rules, copy over the required images and start a headnode
and the requested number of worker instances.

[source, bash]
-----
# Must source the setup script.
source setup.sh
-----

NOTE: Once your instances are started, you can monitor them from the Amazon Web services https://console.aws.amazon.com/ec2/v2/home?region=us-west-2#Instances:[Management Console]


Run the tutorial on the cloud
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Once your cloud resources have been configured and setup, you can run the cloud-tutorial
directly from the headnode. The cloud resources created include a headnode, to which you would
connect to, and the several nodes in worker roles which would do computations in parallel.

To run the tutorial, first connect to the headnode:

[source, bash]
-----
# Connect to the Headnode
# This will have you logged in to the headnode on the cloud
connect headnode

# Go to the cloud tutorial folder
cd cloud-tutorial/swift-cloud-tutorial
-----



NOTE: You must source the setup.sh for the connect command to work

WARNING: Once the cloud instances are started, they start costing money. Remember to shut down
instances using the "dissolve" command.

Supported Operations
^^^^^^^^^^^^^^^^^^^^

Stopping resources
[source, bash]
-----
# Stop workers
stop_n_workers <Number of workers to stop>

# Stop headnode
# If the headnode is stopped, all workers must be stopped and restarted.
stop_headnode

# Terminate all active cloud resources, this will delete every instance active
dissolve
-----

[source, bash]
-----
# Connect to the Headnode
connect headnode
-----

To see resources use :
[source, bash]
-----
list_resources
-----

To ssh to any resource listed :
[source, bash]
-----
# Specify the resource name as listed by list_resources here
# If resource_name is omitted, connect will try to connect to the
# headnode
connect <resource_name>
-----

To stop all resources use from your local machine. Please wait for a couple of minutes
for the command to finish. Confirm that all resources have been removed using
list_resources.  :
[source, bash]
-----
# This will delete the headnode as well as all workers.
# This command will take a few minutes to execute
dissolve
# Use list_resources to check if any resources still linger
list_resource
-----

To add more worker nodes use:
[source, bash]
-----
# The number of nodes you can create is limited by the quota's set by google.
# To increase quotas contact google using the change request form available
# under Your project / Compute engine / Quotas tab in developer console
start_n_more <Number of nodes>

# To remove worker nodes, use following command. If a number greater than the
# number of active workers is specified, all active workers will be deleted. No
# errors will be raised.
stop_n_workers <Number of workers>

# Alternatively update the configs with the total number of nodes you require
# and rerun the setup script
source setup.sh
-----

SSD Disks on worker-instances
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Several EC2 instance especially the C3, M3 series come with SSD disks. The workers
are configured to format these disks to ext4 and combine the first two SSD disks
(/dev/xvdb and /dev/xvdc) as striped disks and mount them on /scratch.


Configuring the worker-instance at boot time
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

There's a provision in the worker, to execute scripts from the user at boot-time.
For example you could mount an S3 bucket if you provide the s3 bucket keys and the
mount command as a script to the WORKER_INIT_SCRIPT config parameter.

Assuming that you are using machines with disks, the worker init scripts will mount
them all on /scratch.

The following script sets up the passwd-s3fs file and mounts the s3 bucket on /s3

File  : mounts3fs.sh
[source, bash]
------
echo "AKIAJLBNUBP6D4U52CXQ:FFsNvJ+Dftor46B++++*THIS_IS_ONLY_AN_EXAMPLE*" > /etc/passwd-s3fs;
chmod 600 /etc/passwd-s3fs
mkdir -p /s3; chmod 777 /s3;
s3fs -o allow_other,gid=2300 swift-s3-test /s3 -ouse_cache=/scratch,parallel_count=25
------

NOTE: Please not that the passwd-s3fs file uses a colon as separator for the Access key id and the Access key

NOTE: The above must be run as *root*, ( use sudo mounts3fs.sh )


Set WORKER_INIT_SCRIPT=/path/to/mounts3fs.sh in cloud-tutorials/ec2/configs before
launching workers.

NOTE: You will have to do this manually on the headnode.


Running Swift using a S3FS mountpoint as a shared-filesystem
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Since this is a slightly more advanced topic, this section expects the users to go over the Amazon
documentation, rather than explain the basics of the AWS S3 aspects. Links will be provided for
reference. Here are the steps involved.

Common
++++++

1. Create an S3 bucket in the Oregon region. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html[Reference]

On the headnode
+++++++++++++++
1. Create /etc/passwd-s3fs containing AWSACCESSKEYID:AWSSECRETACCESSKEY.
2. If you have a disk(EBS volumes) to spare mount that on /scratch to use as an s3fs cache.
3. Execute the following as *root* :

[source, bash]
------
    chmod 600 /etc/passwd-s3fs
    mkdir /s3; chmod 777 /s3;
    # If you have a disk on /scratch
    s3fs -o allow_other,gid=2300 <S3_BUCKET_NAME> /s3 -ouse_cache=/scratch,parallel_count=25
    # If you do not have a disk on /scratch
    s3fs -o allow_other,gid=2300 <S3_BUCKET_NAME> /s3 -oparallel_count=25
------

Once the the s3 bucket has been mounted, you may use df to check the filesystem:

[source, bash]
-----
ubuntu@ip-10-249-4-209:~$ df
Filesystem        1K-blocks    Used    Available Use% Mounted on
/dev/xvda1          8125880 1505292      6184776  20% /
none                      4       0            4   0% /sys/fs/cgroup
udev                 290380      12       290368   1% /dev
tmpfs                 60272     200        60072   1% /run
none                   5120       0         5120   0% /run/lock
none                 301356       0       301356   0% /run/shm
none                 102400       0       102400   0% /run/user
s3fs           274877906944       0 274877906944   0% /s3
-----


On worker nodes
+++++++++++++++

1. If your instance comes with extra disks, format and mount them on /scratch (Refer to [3])
2. Set the WORKER_INIT_SCRIPT to a script which contains the following:
    File: cloud-tutorials/ec2/configs
    [source, bash]
    -----
    WORKER_INIT_SCRIPT=/path/to/script/s3fs_mount.sh
    -----

3. Create the s3fs_mount script:
    File : s3fs_mounts.sh
    [source, bash]
    ------
    echo "AKIAJLBNUBP6D4U52CXQ:FFsNvJ+Dftor46B++++*THIS_IS_ONLY_AN_EXAMPLE*" > /etc/passwd-s3fs;
    chmod 600 /etc/passwd-s3fs
    mkdir -p /s3; chmod 777 /s3;
    # If you have a disk on /scratch
    s3fs -o allow_other,gid=2300 <S3_BUCKET_NAME> /s3 -ouse_cache=/scratch,parallel_count=25
    # If you do not have a disk on /scratch
    s3fs -o allow_other,gid=2300 <S3_BUCKET_NAME> /s3 -oparallel_count=25
    ------

4. Start the workers using source setup.sh or start_n_workers <N_workers>

Changes to swift and swift.conf
+++++++++++++++++++++++++++++++

When running swift on a shared filesystem (S3FS is acting as one), the scripts and folders
involved in the run must also be on a shared filesystem. Secondly, we switch from staging
method local, where all data is local to the headnode and is moved to the workers over the
network, to staging method "direct" where the necessary files reside on a shared filesystem
and can be accessed directly.

There are two noteworthy changes to swift.conf when running Swift from a shared filesystem:
 1. The staging option has changed from "local" to "direct"
 2. The workDirectory must now point at a location on the shared filesystem.
Here's an example :

[source,bash]
-----
sites: cloud-static

site.cloud-static {
    execution {
        type:"coaster-persistent"
        URL: "http://127.0.0.1:50010"
        jobManager: "local:local"
    }
    initialParallelTasks: 20
    maxParallelTasks: 20
    filesystem.type: local
    workDirectory: /s3/swiftwork
    staging: direct
    app.ALL {executable: "*"}

}

lazyErrors: false
executionRetries: 0
keepSiteDir: true
providerStagingPinSwiftFiles: false
alwaysTransferWrapperLog: true
-----


NOTE: You may use any disk available to the instance you are using as a cache which will improve performance,
   and also avoid out of disk errors from situations where s3fs uses you $HOME(default) as cache.

NOTE: I've noticed that performance deteriorates while using the s3fs over extended periods of time. A simple
      remounting usually is sufficient to get it back to speed.

WARNING: Since you now have your AWS keys in a script, ensure that this is not made public!


File  : mounts3fs.sh
[source, bash]
------

chmod 600 /etc/passwd-s3fs
mkdir /s3; chmod 777 /s3;
s3fs -o allow_other,gid=2300 swift-s3-test /s3 -ouse_cache=/scratch,parallel_count=25
------

NOTE: Please not that the passwd-s3fs file uses a colon as separator for the Access key id and the Access key

NOTE: The above must be run as *root*, ( use sudo mounts3fs.sh )




NOTE: When running anything related to S3FS run as root using sudo.


[2] 
[3] http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html [Using EBS volumes]



Troubleshooting
^^^^^^^^^^^^^^^

Permissions for cloud-tutorials folder
++++++++++++++++++++++++++++++++++++++

[source, bash]
------
ubuntu@ip-172-31-25-230:~/cloud-tutorials/swift-cloud-tutorial/part04$ swift p4.swift
Unable to create run directory /home/ubuntu/cloud-tutorials/swift-cloud-tutorial/part04/run001
------

If you see the above you can either chown the entire cloud-tutorials folder to user "ubuntu"
or give relaxed permissions to the directory
[source, bash]
------
# Chown to ubuntu
sudo chown -hR ubuntu /home/ubuntu/cloud-tutorials

# Or you could give very relaxed permissions
sudo chmod -R 777 /home/ubuntu/cloud-tutorials
------


Network firewall issues
+++++++++++++++++++++++

If your workers are unable to connect to the coaster-service on the headnode, the network
rules could be suspect. You can log into the swift-worker-NNN and check the logs from the
workers in /var/log/worker-<hostname>.log and if they show the worker in attempting to connect
state for long periods of time, that is a good indication of connectivity issues. Here are some
steps on the headnode side:

[source,bash]
-----
# connect to the headnode and check if the coaster-service is active on the headnode,
pstree -a

# To test for connectivity start a netcat server in listen mode on port 50001:
nc -l 50001
-----


Once the workers have booted up, connect to one, and try the following steps:

[source,bash]
-----
# connect to the worker, and check if worker.pl is running
# You can also check if the ip address specified on the worker commandline
# of the worker.pl process is the public ip address of the headnode
pstree -a

# To test for connectivity start a netcat client:
nc <IP_of_headnode> 50001

# if the connection was successful, text entered on the commandline will
# show up on the headnode terminal with the netcat server listening
-----





Note: To create your own customised images, you can connect to the swift instances (headnodes/workers) and
modify them. Once done you can create your own images. Documentation on Amazon Machine Images is available
http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html[here]
